{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10983741,"sourceType":"datasetVersion","datasetId":6432859}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q unstructured python-docx aiofiles==22.1.0 docx2txt \"unstructured[pdf]\" playwright pdf2image html2text rank_bm25 codecarbon\n!pip install -q sentence_transformers faiss-cpu transformers accelerate peft bitsandbytes trl langchain langchain_community libmagic python-magic docling\n!apt install poppler-utils","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nRAG Pipeline - Document parsing & Vector Store generation (Docling & LangChain)\n-------------------------------------------------------------------------------\nThis script demonstrates a robust pipeline for building both Docling-based\nand LangChain-based FAISS + BM25 stores from a set of PDF reports.\nIt includes:\n\n1) **Docling-based** parsing:\n   - PDF => HTML/Markdown (via Docling), preserving table structures\n   - Custom chunking that never splits tables\n   - Post-processing to merge small chunks\n\n2) **LangChain-based** parsing:\n   - DirectoryLoader for PDFs\n   - RecursiveCharacterTextSplitter with user-defined chunk size & overlap\n   - Post-processing to merge small chunks as well\n\nBoth approaches produce per-company FAISS + BM25 indexes plus a global index\nthat aggregates all chunks. The final structure on disk looks like this:\n\nFAISS_DB/\n ┣ docling/\n ┃  ┣ SP500/\n ┃  ┃  ┣ Apple/\n ┃  ┃  ┃  ┣ faiss_index…\n ┃  ┃  ┃  ┗ retrievers/keyword_retriever.pkl\n ┃  ┗ GLOBAL_DB/\n ┃     ┣ faiss_index…\n ┃     ┗ retrievers/keyword_retriever.pkl\n ┣ langchain/\n ┃  ┣ SP500/\n ┃  ┃  ┣ Apple/\n ┃  ┃  ┃  ┣ faiss_index…\n ┃  ┃  ┃  ┗ retrievers/keyword_retriever.pkl\n ┃  ┗ GLOBAL_DB/\n ┃     ┣ faiss_index…\n ┃     ┗ retrievers/keyword_retriever.pkl\n\nAuthor: [PLADIFES]\nDate: [17_02_2025]\n\"\"\"\n\nimport os\nimport re\nimport pickle\nfrom pathlib import Path\nfrom typing import List\n\n# ========== 3rd-party libs for text parsing and NLP ==========\nfrom bs4 import BeautifulSoup\nfrom transformers import AutoTokenizer\n\n# ========== LangChain imports ==========\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.schema import Document\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document as LC_Document\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.retrievers import BM25Retriever\n\n# ========== Docling imports for PDF -> HTML conversion ==========\nfrom docling.backend.docling_parse_backend import DoclingParseDocumentBackend\nfrom docling.datamodel.base_models import InputFormat\nfrom docling.datamodel.pipeline_options import (\n    AcceleratorDevice,\n    AcceleratorOptions,\n    PdfPipelineOptions,\n)\nfrom docling.datamodel.settings import settings\nfrom docling.document_converter import DocumentConverter, PdfFormatOption\n\n# ========== Optional utility from langchain_community (for distance strategies) ==========\nfrom langchain_community.vectorstores.utils import DistanceStrategy\n\n# ----------------------- Emissions -----------------------\nfrom codecarbon import EmissionsTracker\n\n###############################################################################\n#                          GLOBAL CONFIGURATION\n###############################################################################\n#: The HuggingFace model name for embeddings\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\" # \"intfloat/multilingual-e5-large\"\n\n#: Chunk size for splitting text (both Docling and LangChain).\n#: You can change this value or even parse from command-line arguments if desired.\nCHUNK_SIZE = 2048 # 512\n\n#: Overlap automatically computed as chunk_size // 10\nCHUNK_OVERLAP = CHUNK_SIZE // 10\n\n#: Minimum chunk size for merging small chunks in the Docling approach\nMIN_CHUNK_SIZE = 512 # 128\n\n#: Whether to also save `.md` output from Docling\nSTORE_MD = True\n\n#: Accelerator device to use for Docling. Options: CPU, CUDA, MPS, AUTO\nACCELERATOR_DEVICE = AcceleratorDevice.CUDA\n\n#: Number of threads for Docling\nNUM_THREADS = 8\n\n#: Root folder containing subfolders (SP500, CAC40, etc.) with PDF reports\ntarget_directory = \"/kaggle/input/sustainability-reports/sustain_reports/\"\n\n#: Directory in which to store all resulting FAISS indexes\ndb_storage_path = \"./FAISS_DB/\"\n\n#: Folder names to look for under `target_directory` (e.g. SP500, CAC40, etc.)\nINDEXES = (\"SP500\", \"CAC40\", \"Other\")\n\n#: Directory to store Docling-extracted .html/.md\nEXTRACTED_TEXT_BASE = \"./docling_extracted\"\n\n#: Docling pipeline options (OCR, table structure, device)\npipeline_options = PdfPipelineOptions()\npipeline_options.accelerator_options = AcceleratorOptions(\n    num_threads=NUM_THREADS,\n    device=ACCELERATOR_DEVICE,\n)\npipeline_options.do_ocr = True\npipeline_options.do_table_structure = True\npipeline_options.table_structure_options.do_cell_matching = True\n\n#: Enable profiling of Docling pipeline timings (optional)\nsettings.debug.profile_pipeline_timings = True\n\n###############################################################################\n#                       GENERIC UTILITY FUNCTIONS\n###############################################################################\ndef ensure_folder_exists(file_path: str) -> None:\n    \"\"\"\n    Ensure the directory for a given file_path exists, creating subdirectories if needed.\n    \"\"\"\n    folder = os.path.dirname(file_path)\n    if folder and not os.path.exists(folder):\n        os.makedirs(folder)\n\ndef replace_extension(file_path: str, new_ext: str) -> str:\n    \"\"\"\n    Replace the extension of `file_path` with `new_ext`.\n    e.g.: replace_extension(\"/path/to/file.pdf\", \".html\") -> \"/path/to/file.html\"\n    \"\"\"\n    base, _ = os.path.splitext(file_path)\n    return base + new_ext\n\ndef get_relative_path(file_path: str, root_folder: str) -> str:\n    \"\"\"\n    Return the path of `file_path` relative to `root_folder`.\n    e.g.: /root/sustain_reports/SP500/Company/file.pdf => SP500/Company/file.pdf\n    \"\"\"\n    return os.path.relpath(file_path, start=root_folder)\n\ndef get_extracted_file_path(\n    pdf_file_path: str,\n    root_pdf_folder: str,\n    base_extracted_folder: str,\n    extension: str = \".html\"\n) -> str:\n    \"\"\"\n    Produce a path for a derived file (e.g. \".html\") that mirrors the PDF’s folder structure.\n    e.g.:\n        pdf_file_path=\"/root/sustain_reports/SP500/Company/report.pdf\"\n        => \"./docling_extracted/SP500/Company/report.html\"\n    \"\"\"\n    relative_path = get_relative_path(pdf_file_path, root_pdf_folder)\n    new_relative_path = replace_extension(relative_path, extension)\n    return os.path.join(base_extracted_folder, new_relative_path)\n\n###############################################################################\n#                       MERGING SMALL CHUNKS (Common)\n###############################################################################\ndef combine_headings(heading_a: str, heading_b: str) -> str:\n    \"\"\"\n    Combine two heading strings (semicolon-separated), ignoring duplicates.\n    e.g. \"Intro;Background\" + \"Background;Conclusion\" => \"Background; Conclusion; Intro\"\n    \"\"\"\n    set_a = {h.strip() for h in heading_a.split(\";\") if h.strip()}\n    set_b = {h.strip() for h in heading_b.split(\";\") if h.strip()}\n    combined = list(set_a.union(set_b))\n    return \"; \".join(sorted(combined))\n\ndef merge_small_chunks(\n    docs: List[Document],\n    min_chunk_size: int,\n    max_chunk_size: int\n) -> List[Document]:\n    \"\"\"\n    Merge consecutive chunks if:\n      - The first chunk is below `min_chunk_size`\n      - The combined size doesn't exceed `max_chunk_size`\n\n    For Docling-based chunks, we track metadata[\"type\"] (table/mixed/paragraph).\n    For LangChain-based, we can default to \"paragraph\" for everything.\n\n    Returns a new list of Documents with merged content & metadata.\n    \"\"\"\n    merged = []\n    i = 0\n    while i < len(docs):\n        current_doc = docs[i]\n        current_len = len(current_doc.page_content)\n\n        # If chunk is large enough or it's the last chunk, just append\n        if current_len >= min_chunk_size or i == (len(docs) - 1):\n            merged.append(current_doc)\n            i += 1\n            continue\n\n        # Attempt to merge with the next chunk\n        next_doc = docs[i + 1]\n        combined_len = current_len + len(next_doc.page_content)\n\n        if combined_len <= max_chunk_size:\n            # Merge them\n            new_content = f\"{current_doc.page_content}\\n{next_doc.page_content}\"\n\n            # Merge headings if present\n            h1 = current_doc.metadata.get(\"heading\", \"\")\n            h2 = next_doc.metadata.get(\"heading\", \"\")\n            all_headings = combine_headings(h1, h2)\n\n            # Determine new chunk type\n            t1 = current_doc.metadata.get(\"type\", \"paragraph\")\n            t2 = next_doc.metadata.get(\"type\", \"paragraph\")\n            if \"table\" in [t1, t2] or \"mixed\" in [t1, t2]:\n                merged_type = \"mixed\"\n            else:\n                merged_type = \"paragraph\"\n\n            # Construct the new doc\n            merged_meta = dict(current_doc.metadata)\n            merged_meta[\"heading\"] = all_headings\n            merged_meta[\"type\"] = merged_type\n            new_doc = Document(page_content=new_content.strip(), metadata=merged_meta)\n\n            merged.append(new_doc)\n            i += 2  # Skip the next doc\n        else:\n            # Can't merge due to size, so just append current and move on\n            merged.append(current_doc)\n            i += 1\n    return merged\n\n###############################################################################\n#                       EMBEDDING + FAISS + BM25 (Common)\n###############################################################################\ndef compute_embeddings(\n    documents: List[Document],\n    embedding_model_name: str = EMBEDDING_MODEL_NAME\n):\n    \"\"\"\n    Build a FAISS index + BM25 from the provided Documents (already chunked).\n    Returns (faiss_db, bm25_retriever).\n    \"\"\"\n    embeddings = HuggingFaceEmbeddings(\n        model_name=embedding_model_name,\n        model_kwargs={\"trust_remote_code\": True},\n        encode_kwargs={\"normalize_embeddings\": True},\n    )\n\n    faiss_db = FAISS.from_documents(\n        documents,\n        embeddings,\n        distance_strategy=DistanceStrategy.COSINE\n    )\n    keyword_retriever = BM25Retriever.from_documents(documents)\n\n    return faiss_db, keyword_retriever\n\ndef save_faiss_and_bm25(\n    faiss_db: FAISS,\n    keyword_retriever: BM25Retriever,\n    output_path: str\n):\n    \"\"\"\n    Persist the FAISS + BM25 retriever to disk under `output_path`.\n    \"\"\"\n    faiss_db.save_local(output_path)\n    retriever_path = os.path.join(output_path, \"retrievers\")\n    os.makedirs(retriever_path, exist_ok=True)\n    with open(os.path.join(retriever_path, \"keyword_retriever.pkl\"), \"wb\") as f:\n        pickle.dump(keyword_retriever, f)\n\n###############################################################################\n#          DOCLING-BASED PIPELINE (Preserve Table Structure)\n###############################################################################\ndef load_or_convert_pdf_with_docling(\n    pdf_file_path: str,\n    root_pdf_folder: str,\n    converter: DocumentConverter\n) -> str:\n    \"\"\"\n    Load or convert a single PDF file to HTML/Markdown via Docling.\n\n    1) Checks if .html / .md with the same base filename exist next to the PDF\n       or in `EXTRACTED_TEXT_BASE`. If yes, load it.\n    2) Otherwise, convert the PDF with Docling, store .html (and .md if STORE_MD).\n    3) Return the HTML/MD string.\n    \"\"\"\n    # Potential local extractions next to the PDF\n    local_html_path = replace_extension(pdf_file_path, \".html\")\n    local_md_path = replace_extension(pdf_file_path, \".md\")\n\n    if os.path.exists(local_html_path):\n        print(f\"[Docling] Found local HTML '{local_html_path}'.\")\n        with open(local_html_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    if os.path.exists(local_md_path):\n        print(f\"[Docling] Found local MD '{local_md_path}'.\")\n        with open(local_md_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    # Potential paths in docling_extracted/...\n    extracted_html_path = get_extracted_file_path(\n        pdf_file_path, root_pdf_folder, EXTRACTED_TEXT_BASE, extension=\".html\"\n    )\n    extracted_md_path = get_extracted_file_path(\n        pdf_file_path, root_pdf_folder, EXTRACTED_TEXT_BASE, extension=\".md\"\n    )\n\n    if os.path.exists(extracted_html_path):\n        print(f\"[Docling] Found existing extracted HTML '{extracted_html_path}'.\")\n        with open(extracted_html_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    if STORE_MD and os.path.exists(extracted_md_path):\n        print(f\"[Docling] Found existing extracted MD '{extracted_md_path}'.\")\n        with open(extracted_md_path, \"r\", encoding=\"utf-8\") as f:\n            return f.read()\n\n    # If none found, convert now with Docling\n    conversion_result = converter.convert(Path(pdf_file_path))\n    doc = conversion_result.document\n    html_text = doc.export_to_html()\n    md_text = doc.export_to_markdown()\n\n    # Save locally\n    ensure_folder_exists(extracted_html_path)\n    with open(extracted_html_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(html_text)\n\n    if STORE_MD:\n        ensure_folder_exists(extracted_md_path)\n        with open(extracted_md_path, \"w\", encoding=\"utf-8\") as f:\n            f.write(md_text)\n\n    # Optional timing debug\n    total_time = conversion_result.timings[\"pipeline_total\"].times\n    print(f\"[Docling] Converted '{pdf_file_path}' in {total_time} seconds.\")\n    return html_text\n\ndef docling_load_documents(folder_path: str, root_pdf_folder: str, company_name: str) -> List[Document]:\n    \"\"\"\n    For all .pdf in `folder_path`, convert them to HTML via Docling,\n    returning a list of (un-chunked) Documents with `.page_content` = HTML.\n    \"\"\"\n    converter = DocumentConverter(\n        format_options={\n            InputFormat.PDF: PdfFormatOption(\n                pipeline_options=pipeline_options\n            )\n        }\n    )\n\n    all_docs = []\n    for fn in os.listdir(folder_path):\n        if fn.lower().endswith(\".pdf\"):\n            pdf_path = os.path.join(folder_path, fn)\n            html_text = load_or_convert_pdf_with_docling(\n                pdf_path, root_pdf_folder, converter\n            )\n            all_docs.append(\n                Document(\n                    page_content=html_text,\n                    metadata={\n                        \"source\": pdf_path,\n                        \"company\": company_name,\n                        \"filename\": fn,\n                    }\n                )\n            )\n    return all_docs\n\ndef extract_table_html(table_element) -> str:\n    \"\"\"\n    Return the <table> HTML exactly as-is.\n    Useful for LLMs that can interpret HTML structure.\n    \"\"\"\n    return str(table_element)\n\ndef split_section_by_length(text: str, max_len: int) -> List[str]:\n    \"\"\"\n    If `text` exceeds max_len, split it on sentence boundaries (.,?!) into smaller chunks.\n    Otherwise, return [text].\n    \"\"\"\n    if len(text) <= max_len:\n        return [text]\n\n    chunks = []\n    buffer = []\n    length_so_far = 0\n    tokens = re.split(r\"([.?!])\", text)  # keep punctuation\n\n    for i in range(0, len(tokens), 2):\n        sentence = tokens[i].strip()\n        if i + 1 < len(tokens):  # re-append punctuation\n            sentence += tokens[i + 1]\n        if (length_so_far + len(sentence)) > max_len:\n            # Flush buffer\n            chunks.append(\" \".join(buffer).strip())\n            buffer = [sentence]\n            length_so_far = len(sentence)\n        else:\n            buffer.append(sentence)\n            length_so_far += len(sentence)\n\n    # Final flush\n    if buffer:\n        chunks.append(\" \".join(buffer).strip())\n    return chunks\n\ndef html_to_structured_chunks(\n    html_text: str,\n    pdf_source: str,\n    company_name: str,\n    file_name: str,\n    chunk_size: int = CHUNK_SIZE,\n    combine_tables_with_heading: bool = True\n) -> List[Document]:\n    \"\"\"\n    Convert HTML into structured chunks, preserving table integrity.\n      - Headings grouped with subsequent paragraphs/tables if combine_tables_with_heading=True\n      - Large sections split by `split_section_by_length`\n      - Then merges small chunks if needed\n    \"\"\"\n    soup = BeautifulSoup(html_text, \"html.parser\")\n\n    # Temporary list to hold big sections\n    section_texts = []\n    current_buffer = \"\"\n\n    def flush_buffer():\n        nonlocal current_buffer\n        txt = current_buffer.strip()\n        if txt:\n            section_texts.append(txt)\n        current_buffer = \"\"\n\n    # 1) Extract headings, paragraphs, tables in order\n    for element in soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\", \"table\"]):\n        if element.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n            # flush old buffer\n            flush_buffer()\n            heading_text = element.get_text(separator=\" \", strip=True)\n            current_buffer = f\"[START HEADING] {heading_text} [END HEADING]\\n\"\n\n        elif element.name == \"p\":\n            paragraph_text = element.get_text(separator=\" \", strip=True)\n            if paragraph_text:\n                current_buffer += paragraph_text + \"\\n\"\n\n        elif element.name == \"table\":\n            table_html_block = extract_table_html(element)\n            wrapped_table = f\"[START TABLE]\\n{table_html_block}\\n[END TABLE]\\n\"\n\n            if combine_tables_with_heading and current_buffer:\n                # Append to the heading buffer\n                current_buffer += wrapped_table\n            else:\n                flush_buffer()\n                current_buffer = wrapped_table\n                flush_buffer()\n\n    flush_buffer()  # leftover\n\n    # 2) Split large sections by length (unless they contain a table marker)\n    chunk_list = []\n    chunk_counter = 0\n    for section_text in section_texts:\n        if \"[START TABLE]\" in section_text:\n            splitted = [section_text]\n        else:\n            splitted = split_section_by_length(section_text, chunk_size)\n\n        for mini_text in splitted:\n            # Identify heading in the chunk\n            heading_match = re.search(r\"\\[START HEADING\\](.*?)\\[END HEADING\\]\", mini_text)\n            heading_str = heading_match.group(1).strip() if heading_match else \"\"\n\n            # Determine chunk type\n            if \"[START TABLE]\" in mini_text:\n                if heading_str:\n                    chunk_type = \"mixed\"\n                else:\n                    chunk_type = \"table\"\n            else:\n                chunk_type = \"paragraph\"\n\n            meta = {\n                \"source\": pdf_source,\n                \"company\": company_name,\n                \"filename\": file_name,\n                \"chunk_index\": chunk_counter,\n                \"type\": chunk_type,\n                \"heading\": heading_str,\n            }\n            chunk_list.append(\n                Document(page_content=mini_text.strip(), metadata=meta)\n            )\n            chunk_counter += 1\n\n    # 3) Merge small chunks\n    merged = merge_small_chunks(\n        chunk_list,\n        min_chunk_size=MIN_CHUNK_SIZE,\n        max_chunk_size=chunk_size\n    )\n    return merged\n\ndef build_faiss_for_company_docling(\n    company_path: str,\n    output_path: str,\n    root_pdf_folder: str\n) -> List[Document]:\n    \"\"\"\n    For a single company's PDF folder, build a Docling-based FAISS + BM25 store:\n      1) Docling => HTML\n      2) Chunk with table-preservation\n      3) Merge small chunks\n      4) Save DB & retriever\n    Returns all chunked Documents for optional global usage.\n    \"\"\"\n    company_name = os.path.basename(company_path)\n    unchunked_docs = docling_load_documents(company_path, root_pdf_folder, company_name)\n\n    if not unchunked_docs:\n        print(f\"[Docling] No PDFs in {company_path}. Skipping.\")\n        return []\n\n    all_chunks = []\n    for doc in unchunked_docs:\n        doc_chunks = html_to_structured_chunks(\n            html_text=doc.page_content,\n            pdf_source=doc.metadata[\"source\"],\n            company_name=doc.metadata[\"company\"],\n            file_name=doc.metadata[\"filename\"],\n            chunk_size=CHUNK_SIZE,\n            combine_tables_with_heading=True\n        )\n        all_chunks.extend(doc_chunks)\n\n    # Build & save\n    db, bm25 = compute_embeddings(all_chunks)\n    save_faiss_and_bm25(db, bm25, output_path)\n    print(f\"[Docling] Saved to '{output_path}' => {len(all_chunks)} chunks.\")\n    return all_chunks\n\ndef build_all_faiss_stores_docling(\n    target_directory: str,\n    db_storage_path: str,\n    indexes: tuple = INDEXES\n):\n    \"\"\"\n    Build docling-based FAISS+BM25 for each company + a global \"docling/GLOBAL_DB\".\n    \"\"\"\n    docling_base_path = os.path.join(db_storage_path, \"docling\")\n    os.makedirs(docling_base_path, exist_ok=True)\n\n    global_chunks = []\n\n    for index_name in indexes:\n        index_path = os.path.join(target_directory, index_name)\n        if not os.path.isdir(index_path):\n            print(f\"[WARNING] Docling: folder '{index_path}' not found.\")\n            continue\n\n        for company_folder in os.listdir(index_path):\n            company_path = os.path.join(index_path, company_folder)\n            if not os.path.isdir(company_path):\n                continue\n\n            output_path = os.path.join(docling_base_path, index_name, company_folder)\n            cdocs = build_faiss_for_company_docling(\n                company_path=company_path,\n                output_path=output_path,\n                root_pdf_folder=target_directory\n            )\n            global_chunks.extend(cdocs)\n\n    # Global\n    if global_chunks:\n        print(f\"[Docling] Building GLOBAL_DB from {len(global_chunks)} chunks.\")\n        db, bm25 = compute_embeddings(global_chunks)\n        global_db_path = os.path.join(docling_base_path, \"GLOBAL_DB\")\n        save_faiss_and_bm25(db, bm25, global_db_path)\n        print(f\"[Docling] GLOBAL_DB saved => {global_db_path}.\")\n    else:\n        print(\"[Docling] No chunks found, skipping global DB.\")\n\n###############################################################################\n#          LANGCHAIN-BASED PIPELINE (DirectoryLoader + Splitter)\n###############################################################################\nMARKDOWN_SEPARATORS = [\n    r\"\\n#{1,6} \",\n    r\"```\\n\",\n    r\"\\n\\*\\*\\*+\\n\",\n    r\"\\n---+\\n\",\n    r\"\\n___+\\n\",\n    r\"\\n\\n\",\n    r\"\\n\",\n    \" \",\n    \"\",\n]\n\ndef load_documents_langchain(files_path: str) -> List[LC_Document]:\n    \"\"\"\n    Load PDF documents from `files_path` using DirectoryLoader (recursively for *.pdf).\n    Returns a list of LC_Document objects (raw text).\n    \"\"\"\n    loader = DirectoryLoader(files_path, glob=\"**/*.pdf\", use_multithreading=True)\n    documents = loader.load()\n    return documents\n\ndef split_documents_langchain(\n    documents: List[LC_Document],\n    tokenizer_name: str = EMBEDDING_MODEL_NAME,\n    chunk_size: int = CHUNK_SIZE,\n    chunk_overlap: int = CHUNK_OVERLAP,\n) -> List[Document]:\n    \"\"\"\n    Split raw LangChain documents into chunked Documents (max length = chunk_size tokens,\n    overlap = chunk_overlap tokens). Then merges small chunks if needed.\n\n    Returns a list of LangChain `Document` objects with updated metadata.\n    \"\"\"\n    # 1) Use a huggingface-based text splitter\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        AutoTokenizer.from_pretrained(tokenizer_name),\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=MARKDOWN_SEPARATORS,\n    )\n\n    # 2) Split\n    splitted_docs = []\n    for doc in documents:\n        splitted_docs.extend(text_splitter.split_documents([doc]))\n\n    # 3) Convert splitted docs => langchain.schema.Document\n    #    We also set `type=paragraph` so that we can unify merging logic\n    converted_docs = []\n    for i, d in enumerate(splitted_docs):\n        meta = dict(d.metadata)\n        meta[\"type\"] = \"paragraph\"  # For merging logic\n        meta[\"chunk_index\"] = i\n        meta[\"heading\"] = meta.get(\"heading\", \"\")  # Empty if not present\n        # Create a new Document\n        converted_docs.append(\n            Document(page_content=d.page_content, metadata=meta)\n        )\n\n    # 4) Merge small chunks using the same function as Docling\n    merged_docs = merge_small_chunks(\n        converted_docs,\n        min_chunk_size=MIN_CHUNK_SIZE,\n        max_chunk_size=chunk_size\n    )\n\n    # Filter out empty docs after merging\n    final_docs = [doc for doc in merged_docs if doc.page_content.strip()]\n    return final_docs\n\ndef build_faiss_for_company_langchain(company_path: str, output_path: str) -> List[Document]:\n    \"\"\"\n    Build a FAISS + BM25 store for PDFs in `company_path`, using:\n      - DirectoryLoader\n      - RecursiveCharacterTextSplitter\n      - merging small chunks\n      - huggingface embeddings\n      - saving to `output_path`\n    Returns the chunked Docs for optional global usage.\n    \"\"\"\n    # Derive the company name from the folder name\n    company_name = os.path.basename(company_path)  # e.g. \"Apple\"\n\n    # 1) Load raw PDF documents using DirectoryLoader\n    raw_docs = load_documents_langchain(company_path)\n    if not raw_docs:\n        print(f\"[LangChain] No PDFs in {company_path}. Skipping.\")\n        return []\n\n    # 2) Inject metadata you want to propagate\n    for d in raw_docs:\n        # The 'source' key typically has the full file path\n        source_path = d.metadata.get(\"source\", \"\")\n        pdf_filename = os.path.basename(source_path)\n        \n        # You can set or overwrite fields in the metadata dict:\n        d.metadata[\"company\"] = company_name     # <-- add the company\n        d.metadata[\"filename\"] = pdf_filename    # <-- add the filename\n        # You can also keep \"source\" (full path) if you like:\n        # d.metadata[\"pdf_path\"] = source_path\n\n    # 3) Split into smaller chunks (with overlap)\n    chunked = split_documents_langchain(\n        raw_docs,\n        tokenizer_name=EMBEDDING_MODEL_NAME,\n        chunk_size=CHUNK_SIZE,\n        chunk_overlap=CHUNK_OVERLAP\n    )\n    if not chunked:\n        print(f\"[LangChain] After splitting, no non-empty chunks in '{company_path}'.\")\n        return []\n\n    # 4) Build embeddings + store\n    db, bm25 = compute_embeddings(chunked)\n    save_faiss_and_bm25(db, bm25, output_path)\n    print(f\"[LangChain] Saved => '{output_path}', with {len(chunked)} chunks.\")\n    return chunked\n\ndef build_all_faiss_stores_langchain(\n    target_directory: str,\n    db_storage_path: str,\n    indexes: tuple = INDEXES\n):\n    \"\"\"\n    Build langchain-based FAISS+BM25 for each company + a global \"langchain/GLOBAL_DB\".\n    \"\"\"\n    langchain_base_path = os.path.join(db_storage_path, \"langchain\")\n    os.makedirs(langchain_base_path, exist_ok=True)\n\n    global_docs = []\n\n    for index_name in indexes:\n        index_path = os.path.join(target_directory, index_name)\n        if not os.path.isdir(index_path):\n            print(f\"[WARNING] LangChain: folder '{index_path}' not found.\")\n            continue\n\n        for company_folder in os.listdir(index_path):\n            company_path = os.path.join(index_path, company_folder)\n            if not os.path.isdir(company_path):\n                continue\n\n            output_path = os.path.join(langchain_base_path, index_name, company_folder)\n            chunked_docs = build_faiss_for_company_langchain(company_path, output_path)\n            global_docs.extend(chunked_docs)\n\n    # Build global DB\n    if global_docs:\n        print(f\"[LangChain] Building GLOBAL_DB from {len(global_docs)} chunks.\")\n        db, bm25 = compute_embeddings(global_docs)\n        global_db_path = os.path.join(langchain_base_path, \"GLOBAL_DB\")\n        save_faiss_and_bm25(db, bm25, global_db_path)\n        print(f\"[LangChain] GLOBAL_DB => '{global_db_path}'.\")\n    else:\n        print(\"[LangChain] No chunks found, skipping global DB.\")\n\n###############################################################################\n#                            MAIN EXECUTION\n###############################################################################\nif __name__ == \"__main__\":\n    \"\"\"\n    Main entry point:\n    1) Build Docling-based indexes (company-level + global).\n    2) Build LangChain-based indexes (company-level + global).\n    Both sets of indexes are stored in `db_storage_path` under:\n       - docling/\n       - langchain/\n    \"\"\"\n    with EmissionsTracker(project_name=\"Climate_Finance_Bench_vector_store_generation\", output_dir=\"/kaggle/working/\") as tracker:\n        \n        print(\"[Pipeline] Building DOCLING-based indexes...\")\n        build_all_faiss_stores_docling(\n            target_directory=target_directory,\n            db_storage_path=db_storage_path,\n            indexes=INDEXES\n        )\n        print(\"[Pipeline] DOCLING-based done.\\n\")\n    \n        print(\"[Pipeline] Building LANGCHAIN-based indexes...\")\n        build_all_faiss_stores_langchain(\n            target_directory=target_directory,\n            db_storage_path=db_storage_path,\n            indexes=INDEXES\n        )\n        print(\"[Pipeline] LANGCHAIN-based done.\\n\")\n    \n        print(\"[DONE] Both indexing methods complete.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n!zip -r file.zip /kaggle/working/\nFileLink(r'file.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}