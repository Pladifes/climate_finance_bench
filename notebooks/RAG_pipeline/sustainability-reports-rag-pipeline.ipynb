{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10983741,"sourceType":"datasetVersion","datasetId":6432859},{"sourceId":11462798,"sourceType":"datasetVersion","datasetId":6574829}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers bitsandbytes accelerate sentence_transformers langchain faiss-cpu rank_bm25 langchain-community quepasa openai anthropic xlsxwriter codecarbon==2.8.4 ecologits[anthropic,openai]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = user_secrets.get_secret(\"HF_TOKEN\")\nos.environ[\"QUEPASA_API_TOKEN\"] = user_secrets.get_secret(\"QUEPASA_TOKEN\")\nos.environ[\"OPENAI_API_KEY\"] = user_secrets.get_secret(\"ILB_OpenAI_key\")\nos.environ[\"ANTHROPIC_API_KEY\"] = user_secrets.get_secret(\"ILB_Anthropic_key\")\nos.environ[\"NEBIUS_API_KEY\"] = user_secrets.get_secret(\"QuePasa_Nebius_key\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git config --global credential.helper store\n!huggingface-cli login --token $HUGGINGFACEHUB_API_TOKEN","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nRAG Pipeline - Query Answering\n------------------------------------\nThis script demonstrates a robust pipeline for:\n\n1) Loading previously built Vector Stores (Docling-based or LangChain-based).\n2) Retrieving relevant chunks using an Ensemble (hybrid: FAISS + BM25) or minimal approach.\n3) Optionally re-ranking with a CrossEncoder (only relevant if retrieval_mode=\"hybrid\").\n4) Generating final answers via different LLM backends:\n   - Local Nemo model (4-bit)\n   - Local LLaMA model (4-bit)\n   - OpenAI GPT-4\n   - Claude 3.5 (Anthropic's API)\n   - QuePasa LLMs (Qwen, Claude Sonnet, DeepSeek, etc.)\n5) Storing results to an Excel file with column names that reflect chosen settings.\n\nAuthor: [PLADIFES]\nDate: [17_02_2025]\n\"\"\"\n\nimport os\nimport time\nimport pickle\nimport re\nimport pandas as pd\nfrom typing import List, Optional, Tuple, Union\n\n# -------------- LangChain & Transformers Imports --------------\nfrom langchain.vectorstores import FAISS\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\nfrom langchain.embeddings.huggingface import HuggingFaceEmbeddings\nfrom langchain.schema import Document\nfrom langchain.prompts import PromptTemplate\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\n\n# -------------- Cross-Encoder Re-ranking --------------\nfrom sentence_transformers import CrossEncoder\n\n# -------------- QuePasa Imports --------------\nimport quepasa\nfrom quepasa.rest import ApiException\n\n# -------------- OpenAI / Anthropic (Claude) --------------\nfrom openai import OpenAI\nimport anthropic\n\n# ----------------------- Emissions -----------------------\nfrom codecarbon import EmissionsTracker\nfrom ecologits import EcoLogits\n\n# -------------- Constants and Configuration --------------\nXLSX_PATH = \"/kaggle/input/cfb-vector-stores/Climate Finance Bench - Dataset.xlsx\"\nOUTPUT_XLSX = \"./Climate Finance Bench - Dataset_with_answers.xlsx\"\nEXCEL_SHEET_NAME = \"Annotations\"\n\n# Root directory containing the two styles of vector stores: \"docling\" and \"langchain\"\nFAISS_DB_ROOT = \"/kaggle/input/cfb-vector-stores/FAISS_DB\"\n\n# If using a shared store, it lives under:\n#   /kaggle/input/cfb-vector-stores/FAISS_DB/docling/GLOBAL_DB  (Docling)\n#   /kaggle/input/cfb-vector-stores/FAISS_DB/langchain/GLOBAL_DB (LangChain)\n# (We will switch based on the vector_store_style parameter.)\nGLOBAL_DB_SUBFOLDER = \"GLOBAL_DB\"\n\nTARGET_PDF_DIRECTORY = \"/kaggle/input/sustainability-reports/sustain_reports/\"\n\nEMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\" # \"intfloat/multilingual-e5-large\"\nCROSS_ENCODER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n\n# ------------------ Retrieval & Ranking Parameters ------------------\nTOP_K = 20  # Maximum # of chunks retrieved from each method (FAISS/BM25).\nKEEP_FIRST_N = 8  # How many of the initially retrieved docs to keep before re-ranking, should be 8\nADD_AFTER_RERANK = 4  # How many to add after re-ranking, should be 4\n# Total final doc count = KEEP_FIRST_N + ADD_AFTER_RERANK = 12\n\n# ------------------ Generation Parameters ------------------\nRAG_MAX_TOKENS = 256  # Max number of tokens (or new tokens) in generation\nRAG_TEMPERATURE = 1\n\n# -------------- Model choices --------------\nMODEL_CHOICE = \"nemo\"  # \"nemo\", \"llama\", \"openai\", \"claude\", \"QuePasa\"\nLLAMA_MODEL_PATH = \"meta-llama/Llama-3.1-8B-Instruct\"\nNEMO_MODEL_PATH = \"mistralai/Mistral-Nemo-Instruct-2407\"\n\n# -------------- Vector Store choices --------------\nVECTOR_STORE_STYLE = \"docling\"  # \"docling\" or \"langchain\"\nSTORE_SCOPE = \"single\"          # \"single\" or \"shared\"\nRETRIEVAL_MODE = \"hybrid\"       # \"minimal\" or \"hybrid\"\nDO_RERANK = True\n\n# -------------- Prompt templates --------------\nRAG_PROMPT_TEMPLATE = \"\"\"\nYou're a documentary assistant.\nAnswer the question indicated between <<< and >>> about the company \"{company}\" based on the context provided below extracted from climate or sustainability reports.\nDo not add any additional notes. \nIf the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.\n\nHere are three examples of the format to follow in your reply as an AI assistant:\n###\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: Yes, the company aims to become net zero by 2010, on their Scopes 2 and 3 emissions.\n\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: No, the company clarifies its will to not achieve net zero.\n\nHuman: Does the company disclose a Transition Plan for FY2023? If yes, highlight its main characteristics.\nAI: Not available in the retrieved information.\n###\n\nHere are excerpts from documents that may contain the information, serving as context to help you:\n###\n{context}\n###\n\nHere's the question asked by the user:\nQuestion: <<< {question} >>>\n\nRead the instructions again:\n\nYou're a documentary assistant.\nAnswer the question indicated between <<< and >>> about the company \"{company}\" based on the context provided below extracted from climate or sustainability reports.\nDo not add any additional notes. \nIf the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.\n\nHere are three examples of the format to follow in your reply as an AI assistant:\n###\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: Yes, the company aims to become net zero by 2010, on their Scopes 2 and 3 emissions.\n\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: No, the company clarifies its will to not achieve net zero.\n\nHuman: Does the company disclose a Transition Plan for FY2023? If yes, highlight its main characteristics.\nAI: Not available in the retrieved information.\n###\n\nHere's the question asked by the user:\nQuestion: <<< {question} >>>\n\n[Your answer here]\n\"\"\"\n\nRAG_PROMPT_TEMPLATE_NEMO = \"\"\"\n[INST]\nYou're a documentary assistant.\nAnswer the question indicated between <<< and >>> about the company \"{company}\" based on the context provided below extracted from climate or sustainability reports.\nDo not add any additional notes. \nIf the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.\n\nHere are three examples of the format to follow in your reply as an AI assistant:\n###\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: Yes, the company aims to become net zero by 2010, on their Scopes 2 and 3 emissions.\n\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: No, the company clarifies its will to not achieve net zero.\n\nHuman: Does the company disclose a Transition Plan for FY2023? If yes, highlight its main characteristics.\nAI: Not available in the retrieved information.\n###\n\nHere's the question asked by the user:\nQuestion: <<< {question} >>>\n\nRead the instructions again:\n\nYou're a documentary assistant.\nAnswer the question indicated between <<< and >>> about the company \"{company}\" based on the context provided below extracted from climate or sustainability reports.\nDo not add any additional notes. \nIf the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.\n\nHere are three examples of the format to follow in your reply as an AI assistant:\n###\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: Yes, the company aims to become net zero by 2010, on their Scopes 2 and 3 emissions.\n\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: No, the company clarifies its will to not achieve net zero.\n\nHuman: Does the company disclose a Transition Plan for FY2023? If yes, highlight its main characteristics.\nAI: Not available in the retrieved information.\n###\n\nHere are excerpts from documents that may contain the information, serving as context to help you:\n###\n{context}\n###\n\nHere's the question asked by the user:\nQuestion: <<< {question} >>>\n\n[Your answer here]\n[/INST]\n\"\"\"\n\nAPI_PROMPT_TEMPLATE = \"\"\"\\\nYou are a documentary assistant.\nAnswer the question about the mentioned company based on the provided context that was extracted from climate or sustainability reports.\nDo not add any additional notes. \nIf the answer to the question is missing from the provided context and you cannot conclude on it on your own, indicate this sincerely.\n\nHere are three examples of the format to follow in your reply:\n###\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: Yes, the company aims to become net zero by 2010, on their Scopes 2 and 3 emissions.\n\nHuman: Does the company have a climate change mitigation objective for FY2023? If yes, specify it.\nAI: No, the company clarifies its will to not achieve net zero.\n\nHuman: Does the company disclose a Transition Plan for FY2023? If yes, highlight its main characteristics.\nAI: Not available in the retrieved information.\n###\n\"\"\"\n\n# --------------------------------------------------------------------------\n#                          CROSS-ENCODER LOGIC\n# --------------------------------------------------------------------------\ndef local_rerank(query: str, docs: List[Document], cross_encoder_model: CrossEncoder) -> List[Document]:\n    \"\"\"\n    Re-rank documents using a local CrossEncoder from SentenceTransformers.\n    Sort them by descending score (best first).\n\n    :param query: The user query/question.\n    :param docs: A list of Document objects to be re-ranked.\n    :param cross_encoder_model: A CrossEncoder model (from sentence-transformers).\n    :return: The re-ranked list of Document objects.\n    \"\"\"\n    if cross_encoder_model is None or not docs:\n        return docs\n    doc_contents = [d.page_content for d in docs]\n    pairs = [(query, c) for c in doc_contents]\n    scores = cross_encoder_model.predict(pairs)\n    scored_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n    return [sd[0] for sd in scored_docs]\n\ndef select_final_documents(\n    question: str,\n    docs: List[Document],\n    cross_encoder_model: Optional[CrossEncoder] = None,\n    top_k: int = TOP_K,\n    keep_first_n: int = KEEP_FIRST_N,\n    add_after_rerank: int = ADD_AFTER_RERANK\n) -> List[Document]:\n    \"\"\"\n    1) Truncate docs to top_k from the retriever.\n    2) Keep the first keep_first_n from the original order.\n    3) Optionally re-rank the truncated list using cross_encoder_model.\n    4) From the re-ranked list, add the top add_after_rerank that are not already in the final set.\n    5) Return the combined unique set.\n\n    :param question: The user question (used for re-ranking).\n    :param docs: A list of Document objects from the retriever.\n    :param cross_encoder_model: If provided, used for re-ranking.\n    :param top_k: Maximum number of docs to consider from initial retrieval.\n    :param keep_first_n: Number of docs to preserve in original order (before re-rank).\n    :param add_after_rerank: Number of docs to add from the re-rank set not already included.\n    :return: A list of selected Document objects.\n    \"\"\"\n    truncated_docs = docs[:top_k]\n    top_n_original = truncated_docs[:keep_first_n]\n\n    # Re-rank if a cross-encoder model is provided\n    if cross_encoder_model:\n        reranked_docs = local_rerank(question, truncated_docs, cross_encoder_model)\n    else:\n        reranked_docs = truncated_docs\n\n    final_docs = []\n    seen_keys = set()\n\n    # Add the original top_n first\n    for d in top_n_original:\n        meta = d.metadata or {}\n        doc_key = (d.page_content, meta.get(\"filename\", \"\"), meta.get(\"page\", \"\"))\n        if doc_key not in seen_keys:\n            final_docs.append(d)\n            seen_keys.add(doc_key)\n\n    # Then add some from the re-ranked set\n    added_count = 0\n    for d in reranked_docs:\n        if added_count >= add_after_rerank:\n            break\n        meta = d.metadata or {}\n        doc_key = (d.page_content, meta.get(\"filename\", \"\"), meta.get(\"page\", \"\"))\n        if doc_key not in seen_keys:\n            final_docs.append(d)\n            seen_keys.add(doc_key)\n            added_count += 1\n\n    return final_docs\n\n\ndef load_cross_encoder(model_name: str = CROSS_ENCODER_MODEL_NAME) -> Optional[CrossEncoder]:\n    \"\"\"\n    Load a cross-encoder model from sentence-transformers for re-ranking.\n\n    :param model_name: The name of the cross-encoder model on HuggingFace.\n    :return: An instance of CrossEncoder or None if load fails.\n    \"\"\"\n    print(f\"[INFO] Loading cross-encoder model: {model_name}\")\n    try:\n        return CrossEncoder(model_name, device='cpu')\n    except Exception as e:\n        print(f\"[ERROR] Could not load cross-encoder: {e}\")\n        return None\n\n# --------------------------------------------------------------------------\n#                          VECTOR STORE LOADING\n# --------------------------------------------------------------------------\ndef load_local_retriever_for_company(\n    company_name: str,\n    vector_store_style: str,\n    retrieval_mode: str = \"hybrid\",\n    embedding_model: str = EMBEDDING_MODEL\n) -> Optional[Union[BM25Retriever, EnsembleRetriever]]:\n    \"\"\"\n    Load a local (per-company) retriever for the given company.  \n      - vector_store_style: \"docling\" or \"langchain\"  \n      - retrieval_mode: \"minimal\" => FAISS only, \"hybrid\" => Ensemble (FAISS+BM25)\n\n    :param company_name: Name of the company folder under SP500/CAC40/Other.\n    :param vector_store_style: Either \"docling\" or \"langchain\".\n    :param retrieval_mode: Either \"minimal\" or \"hybrid\".\n    :param embedding_model: HF embedding model for the FAISS store.\n    :return: A configured Retriever (BM25Retriever, EnsembleRetriever, or None).\n    \"\"\"\n    # Attempt to find it under SP500/CAC40/Other\n    found_path = None\n    for index_name in (\"SP500\", \"CAC40\", \"Other\"):\n        path_candidate = os.path.join(FAISS_DB_ROOT, vector_store_style, index_name, company_name)\n        if os.path.isdir(path_candidate):\n            found_path = path_candidate\n            break\n\n    if not found_path:\n        print(f\"[WARNING] No FAISS folder found for '{company_name}' with style '{vector_store_style}'.\")\n        return None\n\n    embeddings = HuggingFaceEmbeddings(\n        model_name=embedding_model,\n        model_kwargs={\"device\": \"cpu\", \"trust_remote_code\": True},\n        encode_kwargs={\"normalize_embeddings\": True},\n    )\n    # Load the FAISS store\n    db = FAISS.load_local(found_path, embeddings, allow_dangerous_deserialization=True)\n\n    if retrieval_mode == \"minimal\":\n        print(f\"[INFO] Using minimal FAISS retriever for '{company_name}' (style={vector_store_style}).\")\n        return db.as_retriever(k=TOP_K, search_type=\"mmr\")\n\n    # Attempt to load BM25 for the ensemble\n    retriever_pickle = os.path.join(found_path, \"retrievers\", \"keyword_retriever.pkl\")\n    if not os.path.isfile(retriever_pickle):\n        print(f\"[WARNING] No BM25 found for '{company_name}'. Using FAISS only.\")\n        return db.as_retriever(k=TOP_K, search_type=\"mmr\")\n\n    with open(retriever_pickle, \"rb\") as f:\n        bm25_retriever: BM25Retriever = pickle.load(f)\n    bm25_retriever.k = TOP_K\n\n    vector_retriever = db.as_retriever(k=TOP_K, search_type=\"mmr\")\n    ensemble = EnsembleRetriever(\n        retrievers=[vector_retriever, bm25_retriever],\n        weights=[0.75, 0.25]\n    )\n    print(f\"[INFO] Using hybrid (FAISS+BM25) retriever for '{company_name}' (style={vector_store_style}).\")\n    return ensemble\n\ndef load_global_retriever(\n    vector_store_style: str,\n    retrieval_mode: str = \"hybrid\",\n    embedding_model: str = EMBEDDING_MODEL\n) -> Optional[Union[BM25Retriever, EnsembleRetriever]]:\n    \"\"\"\n    Load a shared retriever from docling/GLOBAL_DB or langchain/GLOBAL_DB.\n    If retrieval_mode='minimal', uses FAISS only; else tries to load BM25 as well.\n\n    :param vector_store_style: \"docling\" or \"langchain\".\n    :param retrieval_mode: \"minimal\" or \"hybrid\".\n    :param embedding_model: HF embedding model for the FAISS store.\n    :return: A configured Retriever or None if load fails.\n    \"\"\"\n    global_folder = os.path.join(FAISS_DB_ROOT, vector_store_style, GLOBAL_DB_SUBFOLDER)\n    if not os.path.isdir(global_folder):\n        print(f\"[ERROR] No global DB found at {global_folder}.\")\n        return None\n\n    embeddings = HuggingFaceEmbeddings(\n        model_name=embedding_model,\n        model_kwargs={\"trust_remote_code\": True},\n        encode_kwargs={\"normalize_embeddings\": True},\n    )\n    db = FAISS.load_local(global_folder, embeddings, allow_dangerous_deserialization=True)\n\n    if retrieval_mode == \"minimal\":\n        print(f\"[INFO] Using minimal global retriever (FAISS only) for style={vector_store_style}.\")\n        return db.as_retriever(k=TOP_K, search_type=\"mmr\")\n\n    retriever_pickle = os.path.join(global_folder, \"retrievers\", \"keyword_retriever.pkl\")\n    if not os.path.isfile(retriever_pickle):\n        print(f\"[WARNING] No BM25 found in global store (style={vector_store_style}). Using FAISS only.\")\n        return db.as_retriever(k=TOP_K, search_type=\"mmr\")\n\n    with open(retriever_pickle, \"rb\") as f:\n        bm25_retriever: BM25Retriever = pickle.load(f)\n    bm25_retriever.k = TOP_K\n\n    vector_retriever = db.as_retriever(k=TOP_K, search_type=\"mmr\")\n    ensemble = EnsembleRetriever(\n        retrievers=[vector_retriever, bm25_retriever],\n        weights=[0.75, 0.25]\n    )\n    print(f\"[INFO] Using hybrid shared retriever (FAISS+BM25) for style={vector_store_style}.\")\n    return ensemble\n\n# --------------------------------------------------------------------------\n#                Combine Documents into a Single String\n# --------------------------------------------------------------------------\nDEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(\n    template=(\n        \"<Company: {company}>\\n\"\n        \"<Filename: {filename}>\\n\"\n        \"<Heading: {heading}>\\n\"\n        \"<Content>:\\n{page_content}\"\n    )\n)\n\ndef format_document(doc: Document, document_prompt: PromptTemplate) -> str:\n    \"\"\"\n    Format a single Document using the given PromptTemplate.\n    Fills in placeholders (company, filename, heading, page_content).\n\n    :param doc: The Document to be formatted.\n    :param document_prompt: A PromptTemplate containing placeholders for doc metadata.\n    :return: A formatted string representation of the Document.\n    \"\"\"\n    meta = doc.metadata or {}\n    return document_prompt.format(\n        company=meta.get(\"company\", \"N/A\"),\n        filename=meta.get(\"filename\", \"N/A\"),\n        heading=meta.get(\"heading\", \"N/A\"),\n        page_content=doc.page_content\n    )\n\ndef combine_documents(\n    docs: List[Document],\n    document_prompt: PromptTemplate = DEFAULT_DOCUMENT_PROMPT,\n    document_separator: str = \"\\n\\n### NEW SOURCE ###\\n\\n\"\n) -> str:\n    \"\"\"\n    Combines multiple documents into a single string with structured metadata.\n\n    :param docs: A list of Document objects.\n    :param document_prompt: A PromptTemplate to format each Document.\n    :param document_separator: A string used to separate each Document's content.\n    :return: A single string containing all formatted documents separated accordingly.\n    \"\"\"\n    doc_strings = [format_document(d, document_prompt) for d in docs]\n    return document_separator.join(doc_strings)\n\n# --------------------------------------------------------------------------\n#              Unified Model Loading for Nemo / LLaMA\n# --------------------------------------------------------------------------\ndef load_local_transformers_model(\n    model_name: str,\n    approach: str = \"nemo\",\n    device_map: str = \"auto\",\n    quantize: bool = True\n) -> Tuple[Optional[AutoModelForCausalLM], Optional[AutoTokenizer]]:\n    \"\"\"\n    Load a local Transformers model. If quantize=True, load 4-bit quantized via BitsAndBytes.\n    Otherwise, load in normal precision (e.g. FP16).\n\n    :param model_name: The HF model repo ID (e.g. \"meta-llama/Llama-3.1-8B-Instruct\").\n    :param approach: \"nemo\" or \"llama\" or anything else you want to handle distinctly.\n    :param device_map: \"auto\" or a dictionary for device placement.\n    :param quantize: If True, use 4-bit quantization. If False, load standard HF model.\n    :return: (model, tokenizer), or (None, None) if load fails.\n    \"\"\"\n    if quantize:\n        print(f\"[INFO] Loading local model '{model_name}' with approach='{approach}' in 4-bit quant.\")\n        compute_dtype = getattr(torch, \"float16\")\n        # BitsAndBytes config\n        quant_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=False,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                quantization_config=quant_config,\n                device_map=device_map\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            print(\"[INFO] Local 4-bit model loaded successfully.\")\n            return model, tokenizer\n        except Exception as e:\n            print(f\"[ERROR] Failed to load 4-bit quantized model: {e}\")\n            return None, None\n    else:\n        print(f\"[INFO] Loading local model '{model_name}' with *no* quantization.\")\n        # Optionally force torch_dtype to half or bf16 if you're on GPU. You can also load to CPU.\n        # This example uses FP16 if CUDA is available, else CPU fallback:\n        torch_dtype = torch.float16 if torch.cuda.is_available() else None\n\n        try:\n            model = AutoModelForCausalLM.from_pretrained(\n                model_name,\n                device_map=device_map,\n                torch_dtype=torch_dtype\n            )\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n            print(\"[INFO] Non-quantized model loaded successfully.\")\n            return model, tokenizer\n        except Exception as e:\n            print(f\"[ERROR] Failed to load non-quantized model: {e}\")\n            return None, None\n\n# --------------------------------------------------------------------------\n#     Nubius (DeepSeek R1 and Qwen 2.5), GPT-4o (OpenAI) and Claude 3.5 (Anthropic) Generation\n# --------------------------------------------------------------------------\ndef generate_answer_nubius(\n    question: str,\n    context: str,\n    company: str,\n    nebius_api_key: str,\n    model_name: str = \"deepseek-ai/DeepSeek-R1\",  # or \"Qwen/Qwen2.5-72B-Instruct\"\n    temperature: float = RAG_TEMPERATURE,\n    max_tokens: int = RAG_MAX_TOKENS,\n    system_instructions: str = API_PROMPT_TEMPLATE\n) -> str:\n    \"\"\"\n    Generate an answer using Nebius's API. The interface is very similar to OpenAI's:\n      client = OpenAI(base_url=..., api_key=...)\n      client.chat.completions.create(...)\n    \n    :param question: The user question.\n    :param context: The retrieved context as a string.\n    :param company: Company name for the answer prompt.\n    :param nebius_api_key: Your Nebius API key.\n    :param model_name: The Nebius model name, e.g. 'deepseek-ai/DeepSeek-R1' or 'Qwen/Qwen2.5-72B-Instruct'.\n    :param temperature: Sampling temperature for generation.\n    :param max_tokens: Maximum tokens for the model to generate.\n    :param system_instructions: A system prompt or “instructions” to guide the model.\n    :return: The generated answer string or an error message on failure.\n    \"\"\"\n    if not nebius_api_key:\n        return \"[ERROR] No NEBIUS_API_KEY provided.\"\n\n    # Create a Nebius-based client – same usage as openai.OpenAI, just with the base_url changed\n    client = OpenAI(\n        base_url=\"https://api.studio.nebius.com/v1/\",\n        api_key=nebius_api_key,\n    )\n    \n    # Build the prompt\n    user_prompt = (\n        f\"\"\"\\\n        Here are excerpts from documents about the company {company}:\n        ###\n        {context}\n        ###\n        \n        Here's the question asked by the user:\n        Question: <<< {question} >>>\n        \"\"\")\n    if model_name==\"deepseek-ai/DeepSeek-R1\":\n        max_tokens = max_tokens * 20\n    # API call\n    try:\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": system_instructions},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=temperature,\n            max_tokens=max_tokens)\n        answer_text = response.choices[0].message.content\n        if model_name==\"deepseek-ai/DeepSeek-R1\":\n            answer_text = re.sub(r\"<think>[\\s\\S]*?</think>\", \"\", answer_text, flags=re.MULTILINE).strip()\n        return answer_text\n    except Exception as e:\n        return f\"[ERROR] Nebius {model_name} call failed: {e}\"\n\ndef generate_answer_openai(\n    question: str,\n    context: str,\n    company: str,\n    openai_api_key: str,\n    model_name: str = \"gpt-4o\",\n    temperature: float = RAG_TEMPERATURE,\n    max_tokens: int = RAG_MAX_TOKENS,\n    system_instructions: str = API_PROMPT_TEMPLATE\n) -> str:\n    \"\"\"\n    Generate an answer using OpenAI ChatCompletion (e.g. GPT-4).\n    Requires OPENAI_API_KEY in environment or passed in `openai_api_key`.\n\n    :param question: The user question.\n    :param context: The retrieved context as a string.\n    :param company: Company name for the answer prompt.\n    :param openai_api_key: Your OpenAI API key.\n    :param model_name: The OpenAI model name (e.g. \"gpt-4\").\n    :param temperature: Sampling temperature for generation.\n    :param max_tokens: Maximum tokens for the model to generate.\n    :return: The generated answer string or an error message on failure.\n    \"\"\"\n    \n    if not openai_api_key:\n        return \"[ERROR] No OPENAI_API_KEY provided.\"\n    \n    client = OpenAI()\n    # Build the prompt\n    user_prompt = (\n        f\"\"\"\\\n        Here are excerpts from documents about the company {company}:\n        ###\n        {context}\n        ###\n        \n        Here's the question asked by the user:\n        Question: <<< {question} >>>\n        \"\"\")\n\n    # API call\n    try:\n        response = client.chat.completions.create(\n            model=model_name,\n            messages=[\n                {\"role\": \"system\", \"content\": system_instructions},\n                {\"role\": \"user\", \"content\": user_prompt}\n            ],\n            temperature=temperature,\n            max_tokens=max_tokens)\n        print(f\"Energy consumption: {response.impacts.energy.value} kWh\") \n        print(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\")\n        answer_text = response.choices[0].message.content\n        return answer_text\n    except Exception as e:\n        return f\"[ERROR] OpenAI {model_name} call failed: {e}\"\n\ndef generate_answer_claude(\n    question: str,\n    context: str,\n    company: str,\n    anthropic_api_key: str,\n    model_name: str = \"claude-3-5-sonnet-20241022\",\n    temperature: float = RAG_TEMPERATURE,\n    max_tokens: int = RAG_MAX_TOKENS,\n    system_instructions: str = API_PROMPT_TEMPLATE\n) -> str:\n    \"\"\"\n    Generate an answer using Anthropic's Claude via the official 'anthropic' library.\n    Requires ANTHROPIC_API_KEY in environment or passed in `anthropic_api_key`.\n\n    :param question: The user question.\n    :param context: The retrieved context as a string.\n    :param company: Company name for the answer prompt.\n    :param anthropic_api_key: Your Anthropic API key.\n    :param model_name: The Claude model name (e.g. \"claude-instant-1\").\n    :param temperature: Sampling temperature for generation.\n    :param max_tokens: Maximum tokens for Claude to generate.\n    :return: The generated answer string or an error message on failure.\n    \"\"\"\n    if not anthropic_api_key:\n        return \"[ERROR] No ANTHROPIC_API_KEY provided.\"\n\n    client = anthropic.Client()\n    user_prompt = (\n        f\"\"\"\\\n        Here are excerpts from documents about the company {company}:\n        ###\n        {context}\n        ###\n        \n        Here's the question asked by the user:\n        Question: <<< {question} >>>\n        \"\"\")\n\n    try:\n        response = client.messages.create(\n            system=system_instructions,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"text\",\n                            \"text\": user_prompt\n                        }\n                    ]\n                }\n            ],\n            model=model_name,\n            max_tokens=max_tokens,\n            temperature=temperature)\n        print(f\"Energy consumption: {response.impacts.energy.value} kWh\") \n        print(f\"GHG emissions: {response.impacts.gwp.value} kgCO2eq\")\n        return response.content[0].text\n    except Exception as e:\n        return f\"[ERROR] Claude call failed: {e}\"\n\n# --------------------------------------------------------------------------\n#     Nemo / LLaMA Generation\n# --------------------------------------------------------------------------\ndef generate_answer_rag_nemo(\n    question: str,\n    context: str,\n    company: str,\n    model: Optional[AutoModelForCausalLM],\n    tokenizer: Optional[AutoTokenizer],\n    device: str = \"cuda\",\n    max_new_tokens: int = 512,\n    temperature: float = 0.2\n) -> str:\n    \"\"\"\n    Generate an answer using the Nemo-style prompt (RAG_PROMPT_TEMPLATE_NEMO).\n\n    :param question: The user question.\n    :param context: The retrieved context to help answer.\n    :param company: Company name for the prompt.\n    :param model: The local Nemo model instance (CausalLM).\n    :param tokenizer: The corresponding tokenizer.\n    :param device: The device to use (\"cuda\" or \"cpu\").\n    :param max_new_tokens: Max new tokens to generate.\n    :param temperature: Sampling temperature for generation.\n    :return: The generated answer string or an error message if model not loaded.\n    \"\"\"\n    if model is None or tokenizer is None:\n        return \"[ERROR] Nemo model not loaded.\"\n\n    prompt = RAG_PROMPT_TEMPLATE_NEMO.format(context=context, question=question, company=company)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    prompt_length = inputs['input_ids'].shape[1]\n\n    tokens_out = model.generate(\n        **inputs, \n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        temperature=temperature\n    )\n    raw_output = tokenizer.decode(tokens_out[0][prompt_length:], skip_special_tokens=True).strip()\n    return raw_output\n\ndef generate_answer_rag_llama(\n    question: str,\n    context: str,\n    company: str,\n    model: Optional[AutoModelForCausalLM],\n    tokenizer: Optional[AutoTokenizer],\n    device: str = \"cuda\",\n    max_new_tokens: int = 512,\n    temperature: float = 0.2,\n    system_content=API_PROMPT_TEMPLATE,\n    top_p: float = 0.8,\n) -> str:\n    \"\"\"\n    Generates the final answer using a structured chat approach for Llama-based models.\n\n    :param question: The user question.\n    :param context: The retrieved context to help answer.\n    :param company: Company name for the prompt.\n    :param model: The local LLaMA model instance (CausalLM).\n    :param tokenizer: The corresponding tokenizer.\n    :param device: The device to use (\"cuda\" or \"cpu\").\n    :param max_new_tokens: Maximum tokens to generate after the prompt.\n    :param temperature: Sampling temperature.\n    :param top_p: Nucleus sampling top-p parameter.\n    :return: The generated answer string or an error message if model not loaded.\n    \"\"\"\n    if model is None or tokenizer is None:\n        return \"[ERROR] Llama model or tokenizer not loaded.\"\n\n    # -----------------------------\n    # 1) User content\n    # -----------------------------\n    user_content = (\n        f\"Company: {company}\\n\\n\"\n        f\"Context:\\n{context}\\n\\n\"\n        f\"Question: {question}\"\n    )\n\n    # -----------------------------\n    # 2) Build the conversation\n    # -----------------------------\n    messages = [\n        {\"role\": \"system\", \"content\": system_content},\n        {\"role\": \"user\",   \"content\": user_content},\n    ]\n\n    try:\n        # This is a LLaMA-2/3 style method, may differ by library\n        input_ids = tokenizer.apply_chat_template(\n            messages,\n            add_generation_prompt=True,\n            return_tensors=\"pt\"\n        ).to(device)\n\n        # Use EOS token to stop generation\n        terminators = [\n            tokenizer.eos_token_id,\n            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") if \"<|eot_id|>\" in tokenizer.vocab else None\n        ]\n        terminators = [t for t in terminators if t is not None]\n\n        outputs = model.generate(\n            input_ids=input_ids,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            eos_token_id=terminators,\n            pad_token_id=tokenizer.eos_token_id\n        )\n\n        generated_toks = outputs[0][input_ids.shape[-1]:]\n        decoded = tokenizer.decode(generated_toks, skip_special_tokens=True)\n        return decoded.strip()\n\n    except AttributeError:\n        # If tokenizer doesn't have apply_chat_template, fallback\n        print(\"[WARNING] The tokenizer does not support apply_chat_template. Using a fallback prompt.\")\n        # Use a simpler prompt approach\n        fallback_prompt = (f\"{system_content}\\n\\nUser: {user_content}\\n\\nAssistant:\")\n        inputs = tokenizer(fallback_prompt, return_tensors=\"pt\").to(device)\n        prompt_length = inputs['input_ids'].shape[1]\n        \n        # Use EOS token to stop generation\n        terminators = [\n            tokenizer.eos_token_id,\n            tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") if \"<|eot_id|>\" in tokenizer.vocab else None\n        ]\n        terminators = [t for t in terminators if t is not None]\n        \n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=temperature,\n            top_p=top_p,\n            eos_token_id=terminators,\n            pad_token_id=tokenizer.eos_token_id\n        )\n        raw_output = tokenizer.decode(outputs[0][prompt_length:], skip_special_tokens=True).strip()\n        return raw_output\n\n    except Exception as e:\n        return f\"[ERROR] Llama generation error: {e}\"\n\n# --------------------------------------------------------------------------\n#          QuePasa Path for \"single\" usage\n# --------------------------------------------------------------------------\ndef find_pdf_for_company(company_name: str) -> Optional[str]:\n    \"\"\"\n    Look for a PDF in subfolders SP500/CAC40/Other/[company_name].\n    Return the first PDF found or None.\n\n    :param company_name: The company name to search for.\n    :return: Path to the first PDF found or None if none is found.\n    \"\"\"\n    for index_name in (\"SP500\", \"CAC40\", \"Other\"):\n        path_candidate = os.path.join(TARGET_PDF_DIRECTORY, index_name, company_name)\n        if os.path.isdir(path_candidate):\n            for fn in os.listdir(path_candidate):\n                if fn.lower().endswith(\".pdf\"):\n                    return os.path.join(path_candidate, fn)\n    return None\n\ndef process_quepasa(\n    df: pd.DataFrame,\n    answer_col: str,\n    context_col: str,\n    quepasa_llm: str = \"nebius:Qwen/Qwen2.5-72B-Instruct\",\n    quepasa_use_per_company_domain: bool = True\n) -> pd.DataFrame:\n    \"\"\"\n    For each company in df:\n      1) Finds its PDF under SP500/CAC40/Other/ (if any).\n      2) Upserts it to QuePasa, either under a single domain or a per-company domain.\n      3) Queries the LLM for each question, storing answer/context in the specified columns.\n\n    :param df: The dataframe containing 'Company's name' and 'Question'.\n    :param answer_col: The column name to store answers.\n    :param context_col: The column name to store used context.\n    :param quepasa_llm: The ID of the QuePasa LLM to be used, e.g. \"anthropic:claude-3-5-sonnet-20240620\".\n    :param quepasa_use_per_company_domain: Whether to use a unique domain for each company (True) or a single shared domain (False).\n    :return: Updated dataframe with answers and contexts.\n    \"\"\"\n    configuration = quepasa.Configuration(\n        access_token=os.environ.get(\"QUEPASA_API_TOKEN\", \"YOUR_QUEPASA_API_TOKEN\")\n    )\n    api_client = quepasa.ApiClient(configuration)\n    client = quepasa.DefaultApi(api_client)\n\n    # If you're using a single domain for everyone, define it here:\n    global_domain = \"Climate_Report_Global\"\n\n    for comp_name, group_df in df.groupby(\"Company's name\"):\n        comp_name_str = str(comp_name)\n        # Check if all answers are filled for this group\n        answer_series = group_df[answer_col].fillna(\"\").apply(str.strip)\n        # If *all* answers are non-empty, skip\n        if answer_series.apply(len).all():\n            print(f\"[INFO] All questions for '{comp_name_str}' already answered. Skipping.\")\n            continue\n\n        pdf_path = find_pdf_for_company(comp_name_str)\n        if not pdf_path:\n            print(f\"[WARNING] No PDF for '{comp_name_str}'. Skipping this company.\")\n            continue\n\n        # Decide domain name depending on use_per_company_domain\n        if quepasa_use_per_company_domain:\n            domain = f\"Climate report_{comp_name_str}\"\n        else:\n            # If you want a single domain for everyone (shared vector store), use global_domain\n            domain = global_domain\n        print(f\"\\n[QuePasa] Upserting PDF for '{comp_name_str}': {pdf_path}\")\n        try:\n            response_upsert = client.upsert_files(domain, pdf_path)\n            batch_id = response_upsert.data.batch_id if response_upsert and response_upsert.data else None\n        except ApiException as e:\n            print(f\"[ERROR] QuePasa upsert failed for '{comp_name_str}': {e}\")\n            continue\n\n        # Wait for the ingestion batch to complete\n        while batch_id:\n            time.sleep(10)\n            try:\n                status_resp = client.get_batch_status(batch_id)\n                print(f\"[QuePasa] Batch status for '{comp_name_str}': {status_resp.status}\")\n                if status_resp.status == 'Batch state: done':\n                    break\n            except ApiException as e:\n                print(f\"[ERROR] Checking batch status: {e}\")\n                break\n\n        # Now retrieve & store answers\n        for idx in group_df.index:\n            existing_answer = str(df.at[idx, answer_col]).strip()\n            if existing_answer != \"\":\n                continue  # Skip if already answered\n\n            original_question = str(df.at[idx, \"Question\"]).strip()\n            question = f\"For '{comp_name_str}', {original_question}\"\n            if not question:\n                continue\n\n            try:\n                # We unify the retrieval with top_k = KEEP_FIRST_N + ADD_AFTER_RERANK = 12\n                # so it matches local retrieval approach in chunk count\n                top_k_for_quepasa = KEEP_FIRST_N + ADD_AFTER_RERANK\n\n                ans_resp = client.retrieve_answer(\n                    {\n                        \"question\": question,\n                        \"domain\": domain,\n                        \"llm\": quepasa_llm\n                    }\n                )\n                ctx_resp = client.retrieve_chunks({'question': question, \"domain\": domain, 'top_k': top_k_for_quepasa})\n\n                chunks_list = ctx_resp.data if (ctx_resp and ctx_resp.data) else []\n                context_text = \"\\n\\n#####\".join([ch.text for ch in chunks_list])\n\n                answer_text = ans_resp.data.markdown if (ans_resp and ans_resp.data) else \"\"\n                df.at[idx, answer_col] = answer_text\n                df.at[idx, context_col] = context_text\n\n                # Save after each question\n                df.to_excel(OUTPUT_XLSX, sheet_name=EXCEL_SHEET_NAME, index=False, engine='xlsxwriter')\n                print(f\"Domain: {domain}\")\n                print(f\"Q: {question}\")\n                print(f\"A: {answer_text[:200]}...\")\n\n            except ApiException as e:\n                print(f\"[ERROR] QuePasa API => {e}\")\n                df.at[idx, answer_col] = f\"[ERROR] {e}\"\n                df.to_excel(OUTPUT_XLSX, sheet_name=EXCEL_SHEET_NAME, index=False, engine='xlsxwriter')\n\n    return df\n\n# --------------------------------------------------------------------------\n#                   COLUMN NAME CONSTRUCTION\n# --------------------------------------------------------------------------\ndef shorten_quepasa_llm_name(quepasa_llm: str) -> str:\n    \"\"\"\n    Extract a short label from a QuePasa LLM ID, removing special chars like ':' or '/'.\n\n    :param quepasa_llm: The QuePasa LLM string (e.g. \"anthropic:claude-3-5-sonnet-20240620\").\n    :return: A short name for the LLM (e.g. \"anthropic_claude-3-5-sonnet-20240620\").\n    \"\"\"\n    short_name = quepasa_llm.replace(\":\", \"_\").replace(\"/\", \"_\")\n    return short_name\n\ndef build_column_names(\n    vector_store_style: str,\n    store_scope: str,\n    model_choice: str,\n    retrieval_mode: str,\n    do_rerank: bool,\n    quepasa_llm: str,\n    quepasa_use_per_company_domain: bool\n) -> Tuple[str, str]:\n    \"\"\"\n    Construct (answer_col_name, context_col_name) that reflect the user’s choices.\n\n    - If model_choice == \"QuePasa\", we include the short label for `quepasa_llm` in the column name.\n    - Otherwise, we reflect store_scope, vector_store_style, model_choice, retrieval_mode,\n      and 'withRerank' (if do_rerank==True and retrieval_mode==\"hybrid\").\n\n    :param vector_store_style: \"docling\" or \"langchain\".\n    :param store_scope: \"single\" or \"shared\".\n    :param model_choice: \"QuePasa\", \"nemo\", \"llama\", \"openai\", or \"claude\".\n    :param retrieval_mode: \"minimal\" or \"hybrid\".\n    :param do_rerank: Whether cross-encoder re-ranking is used.\n    :param quepasa_llm: The selected QuePasa LLM string.\n    :param quepasa_use_per_company_domain: If we are using a single store configuration\n    :return: A tuple of (answer_column_name, context_column_name).\n    \"\"\"\n    if model_choice == \"QuePasa\":\n        short_llm_name = shorten_quepasa_llm_name(quepasa_llm)\n        if quepasa_use_per_company_domain:\n            short_llm_name = short_llm_name + \"_single\"\n        else:\n            short_llm_name = short_llm_name + \"_shared\"\n        answer_name = f\"Answer_QuePasa_{short_llm_name}\"\n        context_name = f\"Used_Context_QuePasa_{short_llm_name}\"\n    else:\n        base_parts = [store_scope, vector_store_style, model_choice, retrieval_mode]\n        # add a suffix for reranking only if retrieval_mode=\"hybrid\"\n        if retrieval_mode == \"hybrid\":\n            if do_rerank:\n                base_parts.append(\"withRerank\")\n            else:\n                base_parts.append(\"noRerank\")\n        joined_str = \"_\".join(base_parts)\n        answer_name = f\"Answer_{joined_str}\"\n        context_name = f\"Used_Context_{joined_str}\"\n\n    return answer_name, context_name\n\n# --------------------------------------------------------------------------\n#                             MAIN PIPELINE\n# --------------------------------------------------------------------------\ndef main(\n    vector_store_style: str = VECTOR_STORE_STYLE,  # \"docling\" or \"langchain\"\n    store_scope: str = STORE_SCOPE,                # \"single\" or \"shared\"\n    model_choice: str = MODEL_CHOICE,              # \"nemo_4bit\", \"llama_4bit\", \"llama_full\", \"openai\", \"claude\", \"QuePasa\"\n    retrieval_mode: str = RETRIEVAL_MODE,          # \"minimal\" or \"hybrid\"\n    do_rerank: bool = DO_RERANK,\n    quepasa_llm: str = \"nebius:Qwen/Qwen2.5-72B-Instruct\",\n    quepasa_use_per_company_domain=True\n):\n    \"\"\"\n    Main pipeline for RAG-based query answering, writing answers to Excel.\n\n    Steps:\n      1) Builds dynamic column names based on user’s choices (including any QuePasa model).\n      2) Reads the Excel (XLSX_PATH).\n      3) If using QuePasa, upserts PDFs & retrieves answers from that LLM, stores them in new columns.\n      4) Otherwise, does a local RAG approach with docling/langchain store, optional re-rank,\n         and a local or API-based LLM (Nemo, Llama, OpenAI GPT-4, or Claude).\n      5) Writes final answers back to Excel.\n\n    :param vector_store_style: Which index style to use: \"docling\" or \"langchain\".\n    :param store_scope: Whether to use \"single\" store (per-company) or \"shared\" store (global).\n    :param model_choice: The LLM approach: \"nemo\", \"llama\", \"openai\", \"claude\", or \"QuePasa\".\n    :param retrieval_mode: \"minimal\" => FAISS only, \"hybrid\" => FAISS + BM25 ensemble.\n    :param do_rerank: If True and retrieval_mode==\"hybrid\", use cross-encoder re-ranking.\n    :param quepasa_llm: If model_choice==\"QuePasa\", the LLM ID for the QuePasa service.\n    \"\"\"\n    \n    EcoLogits.init(providers=[\"anthropic\", \"openai\"])\n    \n    # 1) Build dynamic column names\n    answer_col, context_col = build_column_names(\n        vector_store_style=vector_store_style,\n        store_scope=store_scope,\n        model_choice=model_choice,\n        retrieval_mode=retrieval_mode,\n        do_rerank=do_rerank,\n        quepasa_llm=quepasa_llm,\n        quepasa_use_per_company_domain=quepasa_use_per_company_domain\n    )\n\n    # 2) Load the Excel sheet\n    df = pd.read_excel(XLSX_PATH, sheet_name=EXCEL_SHEET_NAME)\n    if \"Company's name\" not in df.columns or \"Question\" not in df.columns:\n        print(\"[ERROR] XLSX must have columns 'Company's name' and 'Question'. Exiting.\")\n        return\n\n    # Ensure the answer/context columns exist\n    if answer_col not in df.columns:\n        df[answer_col] = \"\"\n    if context_col not in df.columns:\n        df[context_col] = \"\"\n\n    # ========== If using QuePasa => skip local vector store logic ==========\n    if model_choice == \"QuePasa\":\n        print(f\"[INFO] Running pipeline with QuePasa LLM = '{quepasa_llm}'.\")\n        # For QuePasa, we ignore local retrieval logic and do direct PDF ingestion + QA\n        df = process_quepasa(\n            df=df,\n            answer_col=answer_col,\n            context_col=context_col,\n            quepasa_llm=quepasa_llm,\n            quepasa_use_per_company_domain=quepasa_use_per_company_domain\n        )\n        print(f\"[DONE] QuePasa pipeline. Results in '{OUTPUT_XLSX}'\")\n        return\n\n    # ========== Otherwise, local RAG approach ==========\n    # 3) If store_scope == \"shared\", load the shared retriever once\n    retriever_shared = None\n    if store_scope == \"shared\":\n        print(\"[INFO] Loading shared/global retriever.\")\n        retriever_shared = load_global_retriever(\n            vector_store_style=vector_store_style,\n            retrieval_mode=retrieval_mode,\n            embedding_model=EMBEDDING_MODEL\n        )\n        if retriever_shared is None:\n            print(\"[ERROR] Could not load shared retriever. Exiting.\")\n            return\n\n    # 4) Load the chosen LLM (only if we need it for Nemo or LLaMA)\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, tokenizer = None, None\n\n    if model_choice == \"nemo_4bit\":\n        model, tokenizer = load_local_transformers_model(\n            NEMO_MODEL_PATH,\n            approach=\"nemo\",\n            device_map=\"auto\",\n            quantize=True\n        )\n    elif model_choice == \"llama_4bit\":\n        model, tokenizer = load_local_transformers_model(\n            LLAMA_MODEL_PATH,\n            approach=\"llama\",\n            device_map=\"auto\",\n            quantize=True\n        )\n    elif model_choice == \"llama_full\":\n        # LLaMA non-quantized\n        model, tokenizer = load_local_transformers_model(\n            LLAMA_MODEL_PATH,\n            approach=\"llama\",\n            device_map=\"auto\",\n            quantize=False\n        )\n    elif model_choice in (\"openai\", \"claude\", \"nebius_deepseek\", \"nebius_qwen\"):\n        pass  # We'll handle generation via API\n    else:\n        print(f\"[ERROR] Model choice '{model_choice}' not recognized. Exiting.\")\n        return\n\n    # 5) If \"hybrid\" + do_rerank, load a cross-encoder for re-ranking\n    cross_encoder_model = None\n    if retrieval_mode == \"hybrid\" and do_rerank:\n        cross_encoder_model = load_cross_encoder(CROSS_ENCODER_MODEL_NAME)\n\n    # 6) Iterate over each row in the Excel\n    for idx, row in df.iterrows():\n        company = str(row[\"Company's name\"]).strip()\n        question = str(row[\"Question\"]).strip()\n        existing_answer = str(row[answer_col]).strip()\n\n        if not question:\n            continue\n        if existing_answer != \"\":\n            # Already answered => skip\n            continue\n\n        # 6a) Retrieve documents (single-company or global)\n        if store_scope == \"single\":\n            # Load local retriever for each company\n            retriever_local = load_local_retriever_for_company(\n                company_name=company,\n                vector_store_style=vector_store_style,\n                retrieval_mode=retrieval_mode,\n                embedding_model=EMBEDDING_MODEL\n            )\n            if not retriever_local:\n                df.at[idx, answer_col] = \"[WARNING] No local retriever found for this company.\"\n                df.to_excel(OUTPUT_XLSX, sheet_name=EXCEL_SHEET_NAME, index=False, engine='xlsxwriter')\n                continue\n            retrieved_docs = retriever_local.get_relevant_documents(question)\n        else:\n            # store_scope == \"shared\"\n            retrieved_docs = retriever_shared.get_relevant_documents(question)\n\n        # 6b) Optionally re-rank\n        final_docs = select_final_documents(\n            question=question,\n            docs=retrieved_docs,\n            cross_encoder_model=cross_encoder_model,\n            top_k=TOP_K,\n            keep_first_n=KEEP_FIRST_N,\n            add_after_rerank=ADD_AFTER_RERANK\n        )\n\n        # 6c) Combine context\n        context_text = combine_documents(final_docs)\n\n        # 6d) Generate answer based on model choice\n        if model_choice == \"nemo_4bit\":\n            answer_text = generate_answer_rag_nemo(\n                question=question,\n                context=context_text,\n                company=company,\n                model=model,\n                tokenizer=tokenizer,\n                device=device,\n                max_new_tokens=RAG_MAX_TOKENS,\n                temperature=RAG_TEMPERATURE\n            )\n        elif model_choice[:5] == \"llama\":\n            answer_text = generate_answer_rag_llama(\n                question=question,\n                context=context_text,\n                company=company,\n                model=model,\n                tokenizer=tokenizer,\n                device=device,\n                max_new_tokens=RAG_MAX_TOKENS,\n                temperature=RAG_TEMPERATURE\n            )\n        elif model_choice == \"openai\":\n            openai_api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n            answer_text = generate_answer_openai(\n                question=question,\n                context=context_text,\n                company=company,\n                openai_api_key=openai_api_key,\n                model_name=\"gpt-4o\",\n                temperature=RAG_TEMPERATURE,\n                max_tokens=RAG_MAX_TOKENS\n            )\n        elif model_choice == \"claude\":\n            anthropic_api_key = os.environ.get(\"ANTHROPIC_API_KEY\", \"\")\n            answer_text = generate_answer_claude(\n                question=question,\n                context=context_text,\n                company=company,\n                anthropic_api_key=anthropic_api_key,\n                model_name=\"claude-3-5-sonnet-20241022\",\n                temperature=RAG_TEMPERATURE,\n                max_tokens=RAG_MAX_TOKENS\n            )\n        elif model_choice == \"nebius_deepseek\":\n            nebius_api_key = os.environ.get(\"NEBIUS_API_KEY\", \"\")\n            answer_text = generate_answer_nubius(\n                question=question,\n                context=context_text,\n                company=company,\n                nebius_api_key=nebius_api_key,\n                model_name=\"deepseek-ai/DeepSeek-R1\",\n                temperature=RAG_TEMPERATURE,\n                max_tokens=RAG_MAX_TOKENS\n            )\n        elif model_choice == \"nebius_qwen\":\n            nebius_api_key = os.environ.get(\"NEBIUS_API_KEY\", \"\")\n            answer_text = generate_answer_nubius(\n                question=question,\n                context=context_text,\n                company=company,\n                nebius_api_key=nebius_api_key,\n                model_name=\"Qwen/Qwen2.5-72B-Instruct\",\n                temperature=RAG_TEMPERATURE,\n                max_tokens=RAG_MAX_TOKENS\n            )\n        else:\n            answer_text = \"[ERROR] Unknown model choice in generation step.\"\n\n        # 6e) Store & save\n        df.at[idx, answer_col] = answer_text\n        df.at[idx, context_col] = context_text\n        df.to_excel(OUTPUT_XLSX, sheet_name=EXCEL_SHEET_NAME, index=False, engine='xlsxwriter')\n\n        # Optional short console print\n        print(f\"\\n== Row {idx} / {company} ==\")\n        print(f\"Q: {question}\")\n        print(f\"A: {answer_text[:200]}...\")\n\n    print(f\"[DONE] Queries processed. Output saved in '{OUTPUT_XLSX}'.\")\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Example usage:\n      python run_inference.py\n      or:\n      python run_inference.py --vector_store_style docling --store_scope single \\\n                              --model_choice QuePasa --retrieval_mode hybrid \\\n                              --do_rerank True --quepasa_llm 'anthropic:claude-3-5-sonnet-20240620'\n    \"\"\"\n    # Just a hard-coded example call here, runnable within a notebook context:\n    # In order to enable the upload of files for QuePasa (not useful if all documents are already uploaded), you need to uncomment the upload part in the process_quepasa function\n    with EmissionsTracker(project_name=\"Climate_Finance_Bench_RAG_pipeline\", output_dir=\"/kaggle/working/\") as tracker:\n        main(\n            vector_store_style=\"langchain\",   # \"docling\" or \"langchain\"\n            store_scope=\"single\",             # \"single\" or \"shared\"\n            model_choice=\"QuePasa\",        # \"nemo_4bit\", \"llama_4bit\", \"llama_full\", \"openai\", \"claude\", \"nebius_deepseek\", \"nebius_qwen\" or \"QuePasa\"\n            retrieval_mode=\"hybrid\",         # \"hybrid\" or \"minimal\"\n            do_rerank=True,\n            quepasa_llm=\"anthropic:claude-3-5-sonnet-20240620\",\n            quepasa_use_per_company_domain=True # True corresponds to one domain on QuePasa's platform for each company, which translates to a single vector store configuration. False corresponds to one domain for all companies, which translates to a shared vector store.\n            ### Separated the Single vs. Shared Vector Store logic between QuePasa's pipeline and ours for clarity's sake on the process\n        )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\n\n!zip -r file.zip /kaggle/working/\nFileLink(r'file.zip')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}